{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e9ff6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorflow.__version__\n",
    "\n",
    "# LSTM\n",
    "\n",
    "import time\n",
    "# Registrar o momento de início da execução\n",
    "start_time = time.time()\n",
    "\n",
    "# Suprimir os warnings\n",
    "from warnings import simplefilter\n",
    "simplefilter(action = 'ignore', category = FutureWarning)\n",
    "\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scikitplot as skplt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# configurar o estilo dos gráficos com o Seaborn\n",
    "sns.set_style('dark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac0d423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando o dataset\n",
    "\n",
    "# Região Amazônica\n",
    "df = pd.read_csv('../df_tran/estados_df/regiao_amazonica_csv/datatran_amazonica_2007_2020_processado.csv')\n",
    "\n",
    "df_teste = pd.read_csv('../df_tran/estados_df/regiao_amazonica_csv/datatran_amazonica_2021_processado.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9626210",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Ver o balanceamento das classes\n",
    "print(df.classificacao_acidente.value_counts())\n",
    "porc = (df[df.classificacao_acidente == 'Não Grave'].shape[0] / df.shape[0]) * 100\n",
    "\n",
    "#print(\"\\nOs acidentes não graves representam {:.2f}% do dataset.\\n\".format(porc))\n",
    "\n",
    "# Plotar gráfico de barras para as Classes\n",
    "sns.countplot('classificacao_acidente', data = df, order = ['Não Grave', 'Grave']);\n",
    "\n",
    "# Nomear os eixos X e Y\n",
    "plt.xlabel('Classificação do acidente', fontsize = 12)\n",
    "plt.ylabel('Total de acidentes', fontsize = 12)\n",
    "\n",
    "plt.text(0.5, 1.05, \"Os acidentes não graves representam {:.2f}% do dataset\\n\\n\".format(porc), \n",
    "         ha='center', \n",
    "         va='center', \n",
    "         transform = plt.gca().transAxes, \n",
    "         fontsize = 14)\n",
    "\n",
    "# Exibir o gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bee9869",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.drop('ufbr', axis = 1, inplace = True)\n",
    "df.drop('km', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e481655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma coluna para representar a ordem original das linhas\n",
    "df['ordem_original'] = range(len(df))\n",
    "\n",
    "# Linhas que contém a variável \"Não Grave\"\n",
    "indices_classificacao_acidente = df[df['classificacao_acidente'] == 'Não Grave'].index\n",
    "\n",
    "# Selecionando aleatoriamente as linhas que serão excluídas\n",
    "indices_excluir = np.random.choice(indices_classificacao_acidente, size = int(len(indices_classificacao_acidente) * 0.95), replace = False)\n",
    "\n",
    "# Excluindo as linhas do df\n",
    "df = df.drop(indices_excluir)\n",
    "\n",
    "# Reordenando o DataFrame com base na coluna 'ordem_original'\n",
    "df = df.sort_values('ordem_original').reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706b5912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropando algumas colunas não relevantes\n",
    "df.drop('feridos_graves', axis = 1, inplace = True)\n",
    "df.drop('uf', axis = 1, inplace = True)\n",
    "df.drop('ordem_original', axis = 1, inplace = True)\n",
    "\n",
    "df_teste.drop('ufbr', axis = 1, inplace = True)\n",
    "df_teste.drop('km', axis = 1, inplace = True)\n",
    "df_teste.drop('feridos_graves', axis = 1, inplace = True)\n",
    "df_teste.drop('uf', axis = 1, inplace = True)\n",
    "\n",
    "# Dropando as linhas duplicadas do dataset\n",
    "df = df.drop_duplicates(ignore_index = True)\n",
    "\n",
    "df_teste = df_teste.drop_duplicates(ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa5ee7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substituindo as observações da classificacao_acidente por 0 e 1\n",
    "\n",
    "df['classificacao_acidente'].replace('Não Grave', int(0), inplace = True)\n",
    "\n",
    "df['classificacao_acidente'].replace('Grave', int(1), inplace = True)\n",
    "\n",
    "\n",
    "df_teste['classificacao_acidente'].replace('Não Grave', int(0), inplace = True)\n",
    "\n",
    "df_teste['classificacao_acidente'].replace('Grave', int(1), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2020359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropando algumas features do df de 2021 para que o número de features em ambos os df sejam equivalentes\n",
    "\n",
    "while df.nunique().sum() != df_teste.nunique().sum():\n",
    "    \n",
    "    # Obtém o índice da última linha do DataFrame\n",
    "    indices = df_teste['causa_acidente'].value_counts().tail(1).index\n",
    "\n",
    "    # Exclui as linhas com os índices obtidos\n",
    "    df_teste.drop(df_teste[df_teste['causa_acidente'].isin(indices)].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa08f4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df\n",
    "#df_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea2b53c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Ver o balanceamento das classes\n",
    "print(df.classificacao_acidente.value_counts())\n",
    "porc = (df[df.classificacao_acidente == 1].shape[0] / df.shape[0]) * 100\n",
    "\n",
    "#print(\"\\nAcidentes Graves representam {:.2f}% do dataset.\\n\".format((df[df.classificacao_acidente == 1].shape[0] / df.shape[0]) * 100))\n",
    "\n",
    "# Plotar gráfico de barras para as Classes\n",
    "sns.countplot('classificacao_acidente', data = df, order = [1, 0]);\n",
    "\n",
    "# Adicionar nomes aos valores 0 e 1 no eixo X\n",
    "plt.xticks(ticks=[0, 1], labels=['Grave', 'Não Grave'])\n",
    "\n",
    "# Nomear os eixos X e Y\n",
    "plt.xlabel('Classificação do acidente', fontsize = 12)\n",
    "plt.ylabel('Total de acidentes', fontsize = 12)\n",
    "\n",
    "plt.text(0.5, 1.05, \"Os acidentes graves representam {:.2f}% do dataset\\n\".format(porc), \n",
    "         ha='center', \n",
    "         va='center', \n",
    "         transform = plt.gca().transAxes, \n",
    "         fontsize = 14)\n",
    "\n",
    "# Exibir o gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bb1187",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ver o balanceamento das classes no conjunto de teste\n",
    "print(df_teste.classificacao_acidente.value_counts())\n",
    "print(\"\\nAcidentes Graves representam {:.2f}% do dataset.\\n\".format((df_teste[df_teste.classificacao_acidente == 1].shape[0] / df_teste.shape[0]) * 100))\n",
    "\n",
    "# Plotar gráfico de barras para as Classes\n",
    "sns.countplot('classificacao_acidente', data = df_teste);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea074d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformando os dados, concatenando as colunas correspondentes e armazenado isso em um df\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehotencoder = OneHotEncoder()\n",
    "\n",
    "# Fase dia\n",
    "fd = ['fase_dia']\n",
    "\n",
    "onehotencoder.fit_transform(df[fd])\n",
    "columns_fd = [fd[0] + ' - ' + cat_name for cat_name in onehotencoder.categories_][0]\n",
    "fdDF = pd.DataFrame(onehotencoder.fit_transform(df[fd]).toarray(), columns = columns_fd)\n",
    "\n",
    "# Condição Meteorológica\n",
    "cm = ['condicao_metereologica']\n",
    "\n",
    "onehotencoder.fit_transform(df[cm])\n",
    "columns_cm = [cm[0] + ' - ' + cat_name for cat_name in onehotencoder.categories_][0]\n",
    "cmDF = pd.DataFrame(onehotencoder.fit_transform(df[cm]).toarray(), columns = columns_cm)\n",
    "\n",
    "# Tipo Acidente\n",
    "ta = ['tipo_acidente']\n",
    "\n",
    "onehotencoder.fit_transform(df[ta])\n",
    "columns_ta = [ta[0] + ' - ' + cat_name for cat_name in onehotencoder.categories_][0]\n",
    "taDF = pd.DataFrame(onehotencoder.fit_transform(df[ta]).toarray(), columns = columns_ta)\n",
    "\n",
    "# Causa Acidente\n",
    "ca = ['causa_acidente']\n",
    "\n",
    "onehotencoder.fit_transform(df[ca])\n",
    "columns_ca = [ca[0] + ' - ' + cat_name for cat_name in onehotencoder.categories_][0]\n",
    "caDF = pd.DataFrame(onehotencoder.fit_transform(df[ca]).toarray(), columns = columns_ca)\n",
    "\n",
    "# Dia Semana\n",
    "ds = ['dia_semana']\n",
    "\n",
    "onehotencoder.fit_transform(df[ds])\n",
    "columns_ds = [ds[0] + ' - ' + cat_name for cat_name in onehotencoder.categories_][0]\n",
    "dsDF = pd.DataFrame(onehotencoder.fit_transform(df[ds]).toarray(), columns = columns_ds)\n",
    "\n",
    "# Sentido Via\n",
    "sv = ['sentido_via']\n",
    "\n",
    "onehotencoder.fit_transform(df[sv])\n",
    "columns_sv = [sv[0] + ' - ' + cat_name for cat_name in onehotencoder.categories_][0]\n",
    "svDF = pd.DataFrame(onehotencoder.fit_transform(df[sv]).toarray(), columns = columns_sv)\n",
    "\n",
    "# Tipo Pista\n",
    "tp = ['tipo_pista']\n",
    "\n",
    "onehotencoder.fit_transform(df[tp])\n",
    "columns_tp = [tp[0] + ' - ' + cat_name for cat_name in onehotencoder.categories_][0]\n",
    "tpDF = pd.DataFrame(onehotencoder.fit_transform(df[tp]).toarray(), columns = columns_tp)\n",
    "\n",
    "# Traçado Via\n",
    "tv = ['tracado_via']\n",
    "\n",
    "onehotencoder.fit_transform(df[tv])\n",
    "columns_tv = [tv[0] + ' - ' + cat_name for cat_name in onehotencoder.categories_][0]\n",
    "tvDF = pd.DataFrame(onehotencoder.fit_transform(df[tv]).toarray(), columns = columns_tv)\n",
    "\n",
    "basePAInputs = pd.concat([fdDF, cmDF, taDF, caDF, dsDF, svDF, tpDF, tvDF], axis = 1)\n",
    "\n",
    "#Outputs\n",
    "\n",
    "# Classificação Acidente\n",
    "cla_acid = ['classificacao_acidente']\n",
    "\n",
    "basePAOutputs = pd.DataFrame(df[cla_acid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20ea648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformando os dados, concatenando as colunas correspondentes e armazenado isso em um df do teste\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehotencoder = OneHotEncoder()\n",
    "\n",
    "# Fase dia\n",
    "fd = ['fase_dia']\n",
    "\n",
    "onehotencoder.fit_transform(df_teste[fd])\n",
    "columns_fd = [fd[0] + ' - ' + cat_name for cat_name in onehotencoder.categories_][0]\n",
    "fdDF_teste = pd.DataFrame(onehotencoder.fit_transform(df_teste[fd]).toarray(), columns = columns_fd)\n",
    "\n",
    "# Condição Meteorológica\n",
    "cm = ['condicao_metereologica']\n",
    "\n",
    "onehotencoder.fit_transform(df_teste[cm])\n",
    "columns_cm = [cm[0] + ' - ' + cat_name for cat_name in onehotencoder.categories_][0]\n",
    "cmDF_teste = pd.DataFrame(onehotencoder.fit_transform(df_teste[cm]).toarray(), columns = columns_cm)\n",
    "\n",
    "# Tipo Acidente\n",
    "ta = ['tipo_acidente']\n",
    "\n",
    "onehotencoder.fit_transform(df_teste[ta])\n",
    "columns_ta = [ta[0] + ' - ' + cat_name for cat_name in onehotencoder.categories_][0]\n",
    "taDF_teste = pd.DataFrame(onehotencoder.fit_transform(df_teste[ta]).toarray(), columns = columns_ta)\n",
    "\n",
    "# Causa Acidente\n",
    "ca = ['causa_acidente']\n",
    "\n",
    "onehotencoder.fit_transform(df_teste[ca])\n",
    "columns_ca = [ca[0] + ' - ' + cat_name for cat_name in onehotencoder.categories_][0]\n",
    "caDF_teste = pd.DataFrame(onehotencoder.fit_transform(df_teste[ca]).toarray(), columns = columns_ca)\n",
    "\n",
    "# Dia Semana\n",
    "ds = ['dia_semana']\n",
    "\n",
    "onehotencoder.fit_transform(df_teste[ds])\n",
    "columns_ds = [ds[0] + ' - ' + cat_name for cat_name in onehotencoder.categories_][0]\n",
    "dsDF_teste = pd.DataFrame(onehotencoder.fit_transform(df_teste[ds]).toarray(), columns = columns_ds)\n",
    "\n",
    "# Sentido Via\n",
    "sv = ['sentido_via']\n",
    "\n",
    "onehotencoder.fit_transform(df_teste[sv])\n",
    "columns_sv = [sv[0] + ' - ' + cat_name for cat_name in onehotencoder.categories_][0]\n",
    "svDF_teste = pd.DataFrame(onehotencoder.fit_transform(df_teste[sv]).toarray(), columns = columns_sv)\n",
    "\n",
    "# Tipo Pista\n",
    "tp = ['tipo_pista']\n",
    "\n",
    "onehotencoder.fit_transform(df_teste[tp])\n",
    "columns_tp = [tp[0] + ' - ' + cat_name for cat_name in onehotencoder.categories_][0]\n",
    "tpDF_teste = pd.DataFrame(onehotencoder.fit_transform(df_teste[tp]).toarray(), columns = columns_tp)\n",
    "\n",
    "# Traçado Via\n",
    "tv = ['tracado_via']\n",
    "\n",
    "onehotencoder.fit_transform(df_teste[tv])\n",
    "columns_tv = [tv[0] + ' - ' + cat_name for cat_name in onehotencoder.categories_][0]\n",
    "tvDF_teste = pd.DataFrame(onehotencoder.fit_transform(df_teste[tv]).toarray(), columns = columns_tv)\n",
    "\n",
    "basePAInputs_teste = pd.concat([fdDF_teste, cmDF_teste, taDF_teste, caDF_teste, dsDF_teste, svDF_teste, tpDF_teste, tvDF_teste], axis = 1)\n",
    "\n",
    "#Outputs\n",
    "\n",
    "# Classificação Acidente\n",
    "cla_acid = ['classificacao_acidente']\n",
    "\n",
    "basePAOutputs_teste = pd.DataFrame(df_teste[cla_acid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0340aa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando os dados de treino e teste\n",
    "\n",
    "# input_training -> treinamento_entrada | input_test -> teste_entrada\n",
    "# output_training -> treinamento_saida | output_test -> teste_saida\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "input_training, input_test, output_training, output_test = train_test_split(basePAInputs, basePAOutputs, test_size = 0.01)\n",
    "\n",
    "input_training_t, input_test_t, output_training_t, output_test_t = train_test_split(basePAInputs_teste, basePAOutputs_teste, test_size = 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64373811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Armazenando os dados de teste e treino em arrays\n",
    "\n",
    "input_training_array = np.array(input_training)\n",
    "input_test_array = np.array(input_test)\n",
    "output_training_array = np.array(output_training)\n",
    "output_test_array = np.array(output_test)\n",
    "\n",
    "input_training_array_t = np.array(input_training_t)\n",
    "input_test_array_t = np.array(input_test_t)\n",
    "output_training_array_t = np.array(output_training_t)\n",
    "output_test_array_t = np.array(output_test_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452d9ebe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# usar técnica under-sampling\n",
    "rus = RandomUnderSampler(sampling_strategy = 'not minority')\n",
    "x_res_test, y_res_test = rus.fit_resample(input_test_array_t, output_test_array_t)\n",
    "\n",
    "# ver o balanceamento das classes\n",
    "print(pd.Series(y_res_test).value_counts())\n",
    "\n",
    "# plotar a nova distribuição de classes\n",
    "sns.countplot(y_res_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651ee01c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_training_array = input_training_array.reshape(-1, 1, len(basePAInputs.columns))\n",
    "input_test_array = input_test_array.reshape(-1, 1, len(basePAInputs.columns))\n",
    "output_training_array = output_training_array.reshape(-1, 1, 1)\n",
    "output_test_array = output_test_array.reshape(-1, 1, 1)\n",
    "\n",
    "input_training_array_t = input_training_array_t.reshape(-1, 1, len(basePAInputs_teste.columns))\n",
    "input_test_array_t = input_test_array_t.reshape(-1, 1, len(basePAInputs_teste.columns))\n",
    "output_training_array_t = output_training_array_t.reshape(-1, 1, 1)\n",
    "output_test_array_t = output_test_array_t.reshape(-1, 1, 1)\n",
    "\n",
    "x_res_test = x_res_test.reshape(-1, 1, len(basePAInputs_teste.columns))\n",
    "y_res_test = y_res_test.reshape(-1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3939c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trazendo manualmente algumas métricas\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23c057e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, LSTM\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(80, return_sequences = True, kernel_initializer = 'random_normal', input_shape = (1, len(basePAInputs.columns))))\n",
    "model.add(LSTM(80, return_sequences = False, kernel_initializer = 'random_normal'))\n",
    "model.add(Dense(1, activation = 'sigmoid', kernel_initializer = 'random_normal'))\n",
    "\n",
    "model.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics = ['accuracy', precision_m, recall_m, f1_m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ca3e49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r = model.fit(input_training_array, output_training_array, batch_size = 250, epochs = 5)\n",
    "\n",
    "metrics_test = model.evaluate(input_test_array_t, output_test_array_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d46f34",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(r.history['loss'], label = 'loss')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5e69a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(input_test_array_t)\n",
    "\n",
    "y_pred = (y_pred > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926b685a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_test_array_t = output_test_array_t.reshape(-1, 1)\n",
    "\n",
    "y_res_test = y_res_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bcce50",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# VN, FN\n",
    "# FP, VP\n",
    "# 0 == Não Grave / 1 == Grave\n",
    "\n",
    "labels = ['Não Grave', 'Grave']\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(output_test_array_t, y_pred)\n",
    "\n",
    "plt.xticks(ticks = [0, 1], labels = labels, size = 12)\n",
    "plt.yticks(ticks = [0, 1], labels = labels, size = 12)\n",
    "\n",
    "plt.ylabel('Label Verdadeira', size = 13, labelpad = 15)\n",
    "plt.xlabel('Label Predita', size = 13, labelpad = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f282b510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registrar o momento de término da execução\n",
    "end_time = time.time()\n",
    "\n",
    "# Calcular o tempo de execução\n",
    "tempo_execucao = end_time - start_time\n",
    "\n",
    "# Exibir o tempo de execução em segundos\n",
    "print(\"Tempo de execução:\", tempo_execucao, \"segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52ccd36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
